# LSTM

本项目在不使用Pytorch包装好的LSTM函数下，实现了在单层LSTM与双层LSTM。
## 自然语言处理课程作业
我们可以通过三个门控-输入门，输出门，遗忘门来实现，相比于传统的RNN加入了门，其实是一种多层次的特征的选择方式
首先解释一下本次作业的目的，手动搭建一个LSTM神经网络来实现一个句子的预测，同时进阶任务是搭建双层LSTM神经网络。     
1990年以来，循环神经网络（RNN）存在梯度爆炸和梯度消失等严重的问题使训练非常困难，应用十分受限。于是1997年人工
智能研究所的主任Jurgen Schmidhuber 提出长短期记忆（LSTM），LSTM使用门控单元及记忆机制大大缓解了早期RNN训练
的问题。于是构建了基于LSTM的语言模型。
## 运行环境
Python版本：3.7.0
Pytorch版本：1.9.0 

## 文件介绍

LSTM.py文件为使用Pytorch内包装好的LSTM函数

LSTMLM.py文件为手动实现**单层LSTM**

doubleLSTMLM.py文件为手动实现**双层LSTM**

## 原理讲解
### step1
在第一个文件中只是单纯的想实现网络的结构和正向传播的过程，并且我想到之前所学习的代码中都会将网络的输入数据X:[batch_size,seq_len,embed_size]的前两个维度进行转置，当时还不太明白，所以在我最初设计的过程中我就没有进行转置，直接是每一个batch中的每一个句子依次输入到网络，相当于就是一个完全串行训练的网络，失去了并行的意义，只实现了LSTM的训练过程。    
具体的网络实现是按照pytorch的官方文档的LSTM神经网络的结构设计的，两次线性变化加上三个sigmoid门和两个tanh门。

### step2
这样设计的网络的训练时间要慢四五倍，认真思考之后我发现转置可以同时训练一整个batch，将输入数据转置为X:[seq_len,batch_size,embed_size]，然后每一次的训练是整个batch的第i个单词，然后就可以达到并行训练的目的，而在代码中其实只需要将输入数据转置，然后将forward函数稍加改动即可。

### step3
在完成上述的设计之后，我决定尝试双层网络的设计。   
双层乃至多层的LSTM神经网络之后，其实就是将上一层的hidden_state作为该层的隐层状态的输入即可，同时还需要注意的是此时的输入门、遗忘门和输出门的三个线性函数的输入不再是原来的[X,hidden_state]，而是[hidden_state,hidden_state_new]，所以线性层的输入维度也需要进行修改。   


同时为了简化代码，我也将网络结构从pytorch的官方文档中的两次线性变换改为了一次线性变换，利用torch.cat将tensor按照其最后一个维度拼接起来.     
具体的实现为:   torch.cat((x,hiddeb_state),1)     
因为本身取得就是X中的第i个，所以在拼接是是按照第一个维度进行拼接，这里有一个误区需要指出：按照哪个维度拼接那么其他维度就需要互相对应。

### step4
至此已经完成了整个作业的设计，但是在设计双层网络的时候，考虑到了课堂上老师讲过多层的LSTM神经网络在训练的时候可以有一定程度的并行性的，比如双层网络的话，从第一层的第二个神经元开始便可以同时训练直至第二层的最后一个神经元，但是通过分析发现：简单的for循环是没有办法达到并行目的，因为理论上两个网络还是串行进行，所以仔细研究后决定采用多进程来实现双层神经网络的并行训练，但是在多进程全局变量的共享上遇到了问题，目前尚未解决。


一开始准备直接使用操作系统线程的概念来实现两个计算并行进行，但是通过学习查找发现，在python的解释器中存在一个互斥锁GIL，即一个进程中只能有一个解释器运行，其他的都处于阻塞状态；基于此遂改为使用进程实现并行的目的，但是也在全局变量的共享上遇到了麻烦，先后尝试进程池、队列等解决，最后止步于损失的反向传播。

### step5
最后决定在双层的基础上改进为双向LSTM神经网络，网络结构不需要做过多修改，只需将正向传播函数中的训练方式改变，第二层的输入仍然为词嵌入向量和隐层状态，但是需要将其进行逆序训练，最后将两层得到的隐层状态进行结合作为模型的输出，并对比其训练效果。


## 心得体会
通过本次项目，我巩固理解了LSTM的基本原理，并动手实现了单层与双层LSTM。同时借此机会，我学会了书写简单的Markdown文本以及github的简单实用。在本次课程中，我不仅学会了统计机器翻译的基本知识，同时也了解了当前机器翻译领域前沿的深度学习技术，开阔了我的视野，受益匪浅。
但是无论是双层还是双向的LSTM神经网络的训练效果和训练时间都逊于单层。我个人认为原因有两个方面：     
第一是训练的轮数不够大，导致收敛的比较慢，所以损失较高；第二是我们的数据样本不够多，而模型复杂度比较高，无法在现有数据上得到很好的收敛效果。   
所以我又针对训练的轮数、优化器、损失函数和学习率等进行了比较，很遗憾并没有看到轮数对于训练结果有明显的改善，但是可以看出双向LSTM的效果要优于双层LSTM；通过学习优化器发现Adam本身就可以达到很到的效果，为了对比选择使用SGD优化器发现效果相差较大；后又将学习率由0.001修改为0.005，发现在损失上会降低一部分，但是并非十分显著。



